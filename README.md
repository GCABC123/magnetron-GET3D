# magnetron.artificial-intelligence-2.0.mincloud.proxia--IMAGINATION-A2

 
ğŸ¤– THE ABC 123 GROUP â„¢ ğŸ¤–

ğŸŒ GENERAL CONSULTING ABC 123 BY OSAROPRIME â„¢.

ğŸŒ ABC 123 USA â„¢

ğŸŒ ABC 123 DESYGN â„¢

ğŸŒ ABC 123 FILMS â„¢

=============================================================

                     ğŸŒ ğ— ğ—”ğ—šğ—¡ğ—˜ğ—§ğ—¥ğ—¢ğ—¡ â„¢ ğŸŒ
                     
ğŸŒ ğ—”ğ—¥ğ—§ğ—œğ—™ğ—œğ—–ğ—œğ—”ğ—Ÿ ğ—œğ—¡ğ—§ğ—˜ğ—Ÿğ—Ÿğ—œğ—šğ—˜ğ—¡ğ—–ğ—˜ ğŸ®.ğŸ¬ â„¢ : IMAGINATION PROXIA (A-2)

For making an IMAGINATION PROXIA that generates a 3D representation of of an object detected in an IMAGE.


*ï¸âƒ£ğŸ“¶ğŸ¤–

+++++++++++++++++++++++++++++++

ASTRAL BODY MINDCLOUD:

PRANIC BODY MINDCLOUD:

INSTINCTIVE MIND MINDCLOUD:

ASTRAL MIND MINDCLOUD: âœ… (**IMAGINATION PROXIA A-2**)

PRANIC MIND MINDCLOUD: 


++++++++++++++++++++++++++++=++


REQUIREMENTS: 

[*] Software Requirements: Python, Pytorch

[*] HARDWARE REQUIREMENTS: fast TPU/GPU (Tensor or Graphics Processing Unit)

[*] DEPENDENCIES: [DOCKERFILE INCLUDED]  


=============================================================


USAGE:

e.g On an **ASTRAL MINDCLOUD** this **PROXIA** can be used to process **INFORMATION** sent to it from an **INSTINCTIVE MIND PROXIA/MINDCLOUD** (OBJECT DETECTION). So for example if the ROBOT encounters an intersting photograph or it sees a location from a particular angle it can take a photograph and then reconstruct the scene virtually from the photograph to better understand the layout of the environment or even what it would be like for a human to be there. ARTIFICIAL INTELLIGENCE 2.0 â„¢ ROBOTS can synthesize new views of places they cannot access but can see from particular angles (very useful). 


e.g To see how an object would like in different lighting situations.



Prerequisite reading:

ğŸŒ ARTIFICIAL INTELLIGENCE PRIMER â„¢: https://www.facebook.com/artificialintelligenceprimer

ğŸŒ ARTIFICIAL INTELLIGENCE 2.0 â„¢ DOCUMENTATION: https://www.facebook.com/aibyabc123/

ğŸŒ MEMBER'S CLUB â„¢: https://www.facebook.com/abc123membersclub/ 

ğŸ‘‘ 

INCLUDED STICKERS/SIGN:

FIND STICKERS HERE: https://bit.ly/3B8D3lE

- PROMOTIONAL MATERIAL FOR ğ— ğ—”ğ—šğ—¡ğ—˜ğ—§ğ—¥ğ—¢ğ—¡ ğ—§ğ—˜ğ—–ğ—›ğ—¡ğ—¢ğ—Ÿğ—¢ğ—šğ—¬ â„¢. (CUSTOM GRAPHICS BY ğ—”ğ—•ğ—– ğŸ­ğŸ®ğŸ¯ ğ——ğ—˜ğ—¦ğ—¬ğ—šğ—¡ â„¢/ğ—¢ğ—¦ğ—”ğ—¥ğ—¢ ğ—›ğ—”ğ—¥ğ—¥ğ—œğ—¢ğ—§ğ—§). THE ğ— ğ—”ğ—šğ—¡ğ—˜ğ—§ğ—¥ğ—¢ğ—¡ ğ—§ğ—˜ğ—–ğ—›ğ—¡ğ—¢ğ—Ÿğ—¢ğ—šğ—¬ â„¢  SYMBOL/LOGO IS A TRADEMARK OF ğ—§ğ—›ğ—˜ ğ—”ğ—•ğ—– ğŸ­ğŸ®ğŸ¯ ğ—šğ—¥ğ—¢ğ—¨ğ—£ â„¢ FOR ğ— ğ—”ğ—šğ—¡ğ—˜ğ—§ğ—¥ğ—¢ğ—¡ ğ—§ğ—˜ğ—–ğ—›ğ—¡ğ—¢ğ—Ÿğ—¢ğ—šğ—¬ â„¢. ğ—§ğ—›ğ—˜ ğ—”ğ—•ğ—– ğŸ­ğŸ®ğŸ¯ ğ—šğ—¥ğ—¢ğ—¨ğ—£ â„¢ SYMBOL/LOGO IS A TRADEMARK OF ğ—§ğ—›ğ—˜ ğ—”ğ—•ğ—– ğŸ­ğŸ®ğŸ¯ ğ—šğ—¥ğ—¢ğ—¨ğ—£ â„¢.

*ï¸âƒ£ğŸ“¶ğŸ¤–

- PROMOTIONAL MATERIAL FOR ğ—”ğ—¥ğ—§ğ—œğ—™ğ—œğ—–ğ—œğ—”ğ—Ÿ ğ—œğ—¡ğ—§ğ—˜ğ—Ÿğ—Ÿğ—œğ—šğ—˜ğ—¡ğ—–ğ—˜ ğŸ®.ğŸ¬ â„¢. (CUSTOM GRAPHICS BY ğ—”ğ—•ğ—– ğŸ­ğŸ®ğŸ¯ ğ——ğ—˜ğ—¦ğ—¬ğ—šğ—¡ â„¢/ğ—¢ğ—¦ğ—”ğ—¥ğ—¢ ğ—›ğ—”ğ—¥ğ—¥ğ—œğ—¢ğ—§ğ—§) THE ğ——ğ—¥ğ—”ğ—šğ—¢ğ—¡ & ğ—–ğ—¥ğ—¢ğ—ªğ—¡ ğŸ‘‘ SYMBOL/LOGO IS A TRADEMARK OF ğ—§ğ—›ğ—˜ ğ—”ğ—•ğ—– ğŸ­ğŸ®ğŸ¯ ğ—šğ—¥ğ—¢ğ—¨ğ—£ â„¢ ASSOCIATED WITH TECHNOLOGY. ğ—§ğ—›ğ—˜ ğ—”ğ—•ğ—– ğŸ­ğŸ®ğŸ¯ ğ—šğ—¥ğ—¢ğ—¨ğ—£ â„¢ SYMBOL/LOGO IS A TRADEMARK OF ğ—§ğ—›ğ—˜ ğ—”ğ—•ğ—– ğŸ­ğŸ®ğŸ¯ ğ—šğ—¥ğ—¢ğ—¨ğ—£ â„¢.

You must display the included stickers/signs (so that it is clearly visible) if you are working with MAGNETRON â„¢ TECHNOLOGY for the purposes of determining whether you want to purchase a technology license or not. This includes but is not limited to public technology displays, trade shows, technology expos, media appearances, Investor events, Computers (exterior), MINDCLOUD STORAGE (e.g server room doors, render farm room doors) etc.

ğŸŒ NOTE: IMAGINATION PROXIA A IS DESCRIBED IN THE ğ—”ğ—¥ğ—§ğ—œğ—™ğ—œğ—–ğ—œğ—”ğ—Ÿ ğ—œğ—¡ğ—§ğ—˜ğ—Ÿğ—Ÿğ—œğ—šğ—˜ğ—¡ğ—–ğ—˜ ğŸ®.ğŸ¬ â„¢ DOCUMENTATION.

ğŸŒ NOTE: ğ—”ğ—¥ğ—§ğ—œğ—™ğ—œğ—–ğ—œğ—”ğ—Ÿ ğ—œğ—¡ğ—§ğ—˜ğ—Ÿğ—Ÿğ—œğ—šğ—˜ğ—¡ğ—–ğ—˜ ğŸ®.ğŸ¬ â„¢ is part of MAGNETRON â„¢ TECHNOLOGY.

ğŸŒ NOTE: REMEMBER ğ—”ğ—¥ğ—§ğ—œğ—™ğ—œğ—–ğ—œğ—”ğ—Ÿ ğ—œğ—¡ğ—§ğ—˜ğ—Ÿğ—Ÿğ—œğ—šğ—˜ğ—¡ğ—–ğ—˜ ğŸ®.ğŸ¬ â„¢ ROBOTS WORK WELL TOGETHER (e.g HIVES, PHALANX, SWARM) AND CAN GATHER DATASETS (WITH EYE CAMERAS, EAR MICROPHONES ETC) FOR DEEP LEARNING (THIS MAKES THEM IDEAL FOR RECONNAISSANCE AS WELL AS ADAPTING TO SITUATION SPECIFIC THREATS IN MILITARY/LAW ENFORCEMENT/PERSONAL SECURITY ENDEAVORS etc). 

ğŸŒ NOTE: REMEMBER ğ—”ğ—¥ğ—§ğ—œğ—™ğ—œğ—–ğ—œğ—”ğ—Ÿ ğ—œğ—¡ğ—§ğ—˜ğ—Ÿğ—Ÿğ—œğ—šğ—˜ğ—¡ğ—–ğ—˜ ğŸ®.ğŸ¬ â„¢ ROBOTS WORK WELL TOGETHER (e.g HIVES, PHALANX, SWARM) MAKING GATHERING IMAGES FOR ADVANCED ROBOTIC NeRF VIEW SYNTHESIS EASY. 


ğŸŒ NOTE: YOU CAN IMPLEMENT HALO FORGE MODE STYLE CONTROL FOR HUMANS TO INTERFACE WITH THIS PROXIA e.g FOR SECURITY SURVEYING. THIS IS AN ADVANCED FORM OF SECURITY/CYBERSECURITY.

ğŸŒ NOTE: THIS PROXIA CAN BE USED TO CONFIRM HOW SOMETHING SHOULD/MIGHT LOOK IN A GIVEN LIGHTING SITUATION.




## GET3D: A Generative Model of High Quality 3D Textured Shapes Learned from Images (NeurIPS 2022)<br><sub>Official PyTorch implementation </sub>

![Teaser image](./docs/assets/get3d_model.png)

**[Paper](https://nv-tlabs.github.io/GET3D/assets/paper.pdf)
, [Project Page](https://nv-tlabs.github.io/GET3D/)**


![Teaser Results](./docs/assets/teaser_result.jpg)

For business inquiries, please visit our website and submit the
form: [NVIDIA Research Licensing](https://www.nvidia.com/en-us/research/inquiries/)

## News

- 2022-09-29: Code released!
- 2022-09-22: Code will be uploaded next week!

## Requirements

* We recommend Linux for performance and compatibility reasons.
* 8 high-end NVIDIA GPUs. We have done all testing and development using V100 or A100
  GPUs.
* 64-bit Python 3.8 and PyTorch 1.9.0. See https://pytorch.org for PyTorch install
  instructions.
* CUDA toolkit 11.1 or later.  (Why is a separate CUDA toolkit installation required? We
  use the custom CUDA extensions from the StyleGAN3 repo. Please
  see [Troubleshooting](https://github.com/NVlabs/stylegan3/blob/main/docs/troubleshooting.md#why-is-cuda-toolkit-installation-necessary))
  .
* We also recommend to install Nvdiffrast following instructions
  from [official repo](https://github.com/NVlabs/nvdiffrast), and
  install [Kaolin](https://github.com/NVIDIAGameWorks/kaolin).
* We provide a [script](./install_get3d.sh) to install packages.

### Server usage through Docker

- Build Docker image

```bash
cd docker
chmod +x make_image.sh
./make_image.sh get3d:v1
```

- Start an interactive docker
  container: `docker run --gpus device=all -it --rm -v YOUR_LOCAL_FOLDER:MOUNT_FOLDER -it get3d:v1 bash`

## Preparing datasets

GET3D is trained on synthetic dataset. We provide rendering scripts for Shapenet. Please
refer to [readme](./render_shapenet_data/README.md) to download shapenet dataset and
render it.

## Train the model

#### Clone the gitlab code and necessary files:

```bash
cd YOUR_CODE_PARH
git clone git@github.com:nv-tlabs/GET3D.git
cd GET3D; mkdir cache; cd cache
wget https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/metrics/inception-2015-12-05.pkl
```

#### Train the model

```bash
cd YOUR_CODE_PATH 
export PYTHONPATH=$PWD:$PYTHONPATH
export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
```

- Train on the unified generator on cars, motorbikes or chair (Improved generator in
  Appendix):

```bash
python train_3d.py --outdir=PATH_TO_LOG --data=PATH_TO_RENDER_IMG --camera_path PATH_TO_RENDER_CAMERA --gpus=8 --batch=32 --gamma=40 --data_camera_mode shapenet_car  --dmtet_scale 1.0  --use_shapenet_split 1  --one_3d_generator 1  --fp32 0
python train_3d.py --outdir=PATH_TO_LOG --data=PATH_TO_RENDER_IMG --camera_path PATH_TO_RENDER_CAMERA --gpus=8 --batch=32 --gamma=80 --data_camera_mode shapenet_motorbike  --dmtet_scale 1.0  --use_shapenet_split 1  --one_3d_generator 1  --fp32 0
python train_3d.py --outdir=PATH_TO_LOG --data=PATH_TO_RENDER_IMG --camera_path PATH_TO_RENDER_CAMERA --gpus=8 --batch=32 --gamma=400 --data_camera_mode shapenet_chair  --dmtet_scale 0.8  --use_shapenet_split 1  --one_3d_generator 1  --fp32 0
```

- If want to train on seperate generators (main Figure in the paper):

```bash
python train_3d.py --outdir=PATH_TO_LOG --data=PATH_TO_RENDER_IMG --camera_path PATH_TO_RENDER_CAMERA --gpus=8 --batch=32 --gamma=40 --data_camera_mode shapenet_car  --dmtet_scale 1.0  --use_shapenet_split 1  --one_3d_generator 0
python train_3d.py --outdir=PATH_TO_LOG --data=PATH_TO_RENDER_IMG --camera_path PATH_TO_RENDER_CAMERA --gpus=8 --batch=32 --gamma=80 --data_camera_mode shapenet_motorbike  --dmtet_scale 1.0  --use_shapenet_split 1  --one_3d_generator 0
python train_3d.py --outdir=PATH_TO_LOG --data=PATH_TO_RENDER_IMG --camera_path PATH_TO_RENDER_CAMERA --gpus=8 --batch=32 --gamma=3200 --data_camera_mode shapenet_chair  --dmtet_scale 0.8  --use_shapenet_split 1  --one_3d_generator 0
```

If want to debug the model first, reduce the number of gpus to 1 and batch size to 4 via:

```bash
--gpus=1 --batch=4
```

## Inference

### Inference on a pretrained model for visualization

- Inference could operate on a single GPU with 16 GB memory.

```bash
python train_3d.py --outdir=save_inference_results/shapenet_car  --gpus=1 --batch=4 --gamma=40 --data_camera_mode shapenet_car  --dmtet_scale 1.0  --use_shapenet_split 1  --one_3d_generator 1  --fp32 0 --inference_vis 1 --resume_pretrain MODEL_PATH
python train_3d.py --outdir=save_inference_results/shapenet_chair  --gpus=1 --batch=4 --gamma=40 --data_camera_mode shapenet_chair  --dmtet_scale 0.8  --use_shapenet_split 1  --one_3d_generator 1  --fp32 0 --inference_vis 1 --resume_pretrain MODEL_PATH
python train_3d.py --outdir=save_inference_results/shapenet_motorbike  --gpus=1 --batch=4 --gamma=40 --data_camera_mode shapenet_motorbike  --dmtet_scale 1.0  --use_shapenet_split 1  --one_3d_generator 1  --fp32 0 --inference_vis 1 --resume_pretrain MODEL_PATH
```

- To generate mesh with textures, add one option to the inference
  command: `--inference_to_generate_textured_mesh 1`

- To generate the results with latent code interpolation, add one option to the inference
  command: `--inference_save_interpolation 1`

### Evluation metrics

##### Compute FID

- To evaluate the model with FID metric, add one option to the inference
  command: `--inference_compute_fid 1`

##### Compute COV & MMD scores for LFD & CD

- First generate 3D objects for evaluation, add one option to the inference
  command: `--inference_generate_geo 1`
- Following [README](./evaluation_scripts/README.md) to compute metrics.

## License

Copyright &copy; 2022, NVIDIA Corporation & affiliates. All rights reserved.

This work is made available under
the [Nvidia Source Code License](https://github.com/nv-tlabs/GET3D/blob/master/LICENSE.txt)
.

## Broader Information

GET3D builds upon several previous works:

- [Learning Deformable Tetrahedral Meshes for 3D Reconstruction (NeurIPS 2020)](https://nv-tlabs.github.io/DefTet/)
- [Deep Marching Tetrahedra: a Hybrid Representation for High-Resolution 3D Shape Synthesis (NeurIPS 2021)](https://nv-tlabs.github.io/DMTet/)
- [Extracting Triangular 3D Models, Materials, and Lighting From Images (CVPR 2022)](https://nvlabs.github.io/nvdiffrec/)
- [DIB-R++: Learning to Predict Lighting and Material with a Hybrid Differentiable Renderer (NeurIPS 2021)](https://nv-tlabs.github.io/DIBRPlus/)
- [Nvdiffrast â€“ Modular Primitives for High-Performance Differentiable Rendering (SIGRAPH Asia 2020)](https://nvlabs.github.io/nvdiffrast/)

## Citation

```latex
@inproceedings{gao2022get3d,
title={GET3D: A Generative Model of High Quality 3D Textured Shapes Learned from Images},
author={Jun Gao and Tianchang Shen and Zian Wang and Wenzheng Chen and Kangxue Yin
and Daiqing Li and Or Litany and Zan Gojcic and Sanja Fidler},
booktitle={Advances In Neural Information Processing Systems},
year={2022}
}
```
